# -*- coding: utf-8 -*-
"""BagOfWord.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OjKtNbkI7TAR4Q-344BaH5gcbj-0G0Db
"""

import gdown
import nltk
import re
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

nltk.download('punkt_tab')
nltk.download('wordnet')

url = "https://drive.google.com/uc?id=1vnf0SL2ucnABzL5nWUbZRvyto8rS2rMz"
output = "imdb.csv"
gdown.download(url, output)

class BagOfWord:
  def __init__(self, df):
    self.df = df
    
  def process(self, feat_name=None, target_name=None, max_feature=100):
    vectorizer = CountVectorizer(
      stop_words = "english",
      max_features = max_feature
    )
    x = vectorizer.fit_transform(self.df[feat_name])
    count_vect_df = pd.DataFrame(x.todense(), columns=vectorizer.get_feature_names_out())

    #Replace positive by 1 and negative by 2
    self.df[target_name] = self.df[target_name].replace({"positive":1, "negative":0})

    #Concat dataframe
    self.df = pd.concat([self.df[target_name], count_vect_df], axis=1)

    #Split data
    y = self.df[target_name]
    X = self.df.drop([target_name],axis = 1)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

    self.X_train = X_train
    self.X_test = X_test
    self.y_train = y_train
    self.y_test = y_test
    
  def train(self):
    self.model = LogisticRegression()
    self.model.fit(self.X_train, self.y_train)

  def predict(self):
    y_train_pred = self.model.predict(self.X_train)
    y_test_pred = self.model.predict(self.X_test)
    
    train_accuracy = accuracy_score(self.y_train, y_train_pred)
    test_accuracy = accuracy_score(self.y_test, y_test_pred)

    print(f"\n\nTraining accuracy: {train_accuracy}")
    print(f"Test accuracy: {test_accuracy}")



def clean_data(text):
  text = text.lower()
  text = re.sub(r"\W"," ",text)
  text = re.sub(r"\d","",text)
  lemmatizer = WordNetLemmatizer()
  text = lemmatizer.lemmatize(text)

  return text

def main():
  data = pd.read_csv("imdb.csv")
  data["review"] = data["review"].apply(lambda x: clean_data(x))

  bag = BagOfWord(data)
  bag.process("review", "sentiment", 1000)

  #Train and evaluate model
  bag.train()
  bag.predict()


#Entry point for the script
if __name__ == "__main__":
    main()